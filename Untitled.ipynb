{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------- Options ---------------\n",
      "                    align: True                          \t[default: False]\n",
      "             aspect_ratio: 1.0                           \n",
      "      cache_filelist_read: False                         \n",
      "     cache_filelist_write: False                         \n",
      "          checkpoints_dir: ./checkpoints                 \n",
      "               chunk_size: [1]                           \t[default: None]\n",
      "   contain_dontcare_label: False                         \n",
      "                crop_size: 256                           \n",
      "                  dataset: example                       \t[default: ms1m,casia]\n",
      "             dataset_mode: allface                       \n",
      "             device_count: 2                             \t[default: 8]\n",
      "          display_winsize: 256                           \n",
      "             erode_kernel: 21                            \n",
      "                  gpu_ids: 0,1                           \t[default: 0]\n",
      "             heatmap_size: 2.5                           \t[default: 3]\n",
      "                 how_many: inf                           \n",
      "                init_type: xavier                        \n",
      "            init_variance: 0.02                          \n",
      "                  isTrain: False                         \t[default: None]\n",
      "               label_mask: True                          \t[default: False]\n",
      "                 label_nc: 5                             \n",
      "           landmark_align: False                         \n",
      "                 list_end: 10                            \t[default: inf]\n",
      "                 list_num: 0                             \n",
      "               list_start: 0                             \n",
      "       load_from_opt_file: False                         \n",
      "                load_size: 256                           \n",
      "         max_dataset_size: 9223372036854775807           \n",
      "                    model: rotatespade                   \t[default: rotate]\n",
      "                multi_gpu: True                          \t[default: False]\n",
      "                 nThreads: 3                             \t[default: 1]\n",
      "                     name: mesh2face                     \n",
      "                    names: rs_model                      \t[default: rs_ijba3]\n",
      "                      nef: 16                            \n",
      "                     netG: rotatespade                   \t[default: rotate]\n",
      "                      ngf: 64                            \n",
      "                  no_flip: True                          \n",
      "     no_gaussian_landmark: True                          \t[default: False]\n",
      "              no_instance: True                          \n",
      "         no_pairing_check: False                         \n",
      "                   norm_D: spectralinstance              \n",
      "                   norm_E: spectralinstance              \n",
      "                   norm_G: spectralsyncbatch             \t[default: spectralinstance]\n",
      "                output_nc: 3                             \n",
      "                    phase: test                          \n",
      "              pitch_poses: None                          \n",
      "              posesrandom: False                         \n",
      "          preprocess_mode: scale_width_and_crop          \n",
      "            render_thread: 1                             \t[default: 2]\n",
      "resnet_initial_kernel_size: 7                             \n",
      "       resnet_kernel_size: 3                             \n",
      "          resnet_n_blocks: 9                             \n",
      "      resnet_n_downsample: 4                             \n",
      "              results_dir: ./results/                    \n",
      "                save_path: ./results/                    \n",
      "           serial_batches: True                          \n",
      "                  trainer: rotate                        \n",
      "              which_epoch: latest                        \n",
      "                yaw_poses: [0.0, 30.0]                   \t[default: None]\n",
      "----------------- End -------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No traceback available to show.\n"
     ]
    }
   ],
   "source": [
    "%tb\n",
    "import torch.multiprocessing as multiprocessing\n",
    "multiprocessing.set_start_method('spawn', force=True)\n",
    "from models.networks.sync_batchnorm import DataParallelWithCallback\n",
    "import sys\n",
    "import numpy as np\n",
    "import os\n",
    "import data\n",
    "from util.iter_counter import IterationCounter\n",
    "from options.test_options import TestOptions\n",
    "from models.test_model import TestModel\n",
    "from util.visualizer import Visualizer\n",
    "from util import html, util\n",
    "from torch.multiprocessing import Process, Queue, Pool\n",
    "from data.data_utils import init_parallel_jobs\n",
    "from skimage import transform as trans\n",
    "import cv2\n",
    "import time\n",
    "import torch\n",
    "from models.networks.rotate_render import TestRender\n",
    "\n",
    "\n",
    "def create_path(a_path, b_path):\n",
    "    name_id_path = os.path.join(a_path, b_path)\n",
    "    if not os.path.exists(name_id_path):\n",
    "        os.makedirs(name_id_path)\n",
    "    return name_id_path\n",
    "\n",
    "\n",
    "def create_paths(save_path, img_path, foldername='orig', folderlevel=2, pose='0'):\n",
    "    save_rotated_path_name = create_path(save_path, foldername)\n",
    "\n",
    "    path_split = img_path.split('/')\n",
    "    rotated_file_savepath = save_rotated_path_name\n",
    "    for level in range(len(path_split) - folderlevel, len(path_split)):\n",
    "        file_name = path_split[level]\n",
    "        if level == len(path_split) - 1:\n",
    "            file_name = str(pose) + '_' + file_name\n",
    "        rotated_file_savepath = os.path.join(rotated_file_savepath, file_name)\n",
    "    return rotated_file_savepath\n",
    "\n",
    "def affine_align(img, landmark=None, **kwargs):\n",
    "    M = None\n",
    "    src = np.array([\n",
    "     [38.2946, 51.6963],\n",
    "     [73.5318, 51.5014],\n",
    "     [56.0252, 71.7366],\n",
    "     [41.5493, 92.3655],\n",
    "     [70.7299, 92.2041] ], dtype=np.float32 )\n",
    "    src=src * 224 / 112\n",
    "\n",
    "    dst = landmark.astype(np.float32)\n",
    "    tform = trans.SimilarityTransform()\n",
    "    tform.estimate(dst, src)\n",
    "    M = tform.params[0:2,:]\n",
    "    warped = cv2.warpAffine(img, M, (224, 224), borderValue = 0.0)\n",
    "    return warped\n",
    "\n",
    "def landmark_68_to_5(t68):\n",
    "    le = t68[36:42, :].mean(axis=0, keepdims=True)\n",
    "    re = t68[42:48, :].mean(axis=0, keepdims=True)\n",
    "    no = t68[31:32, :]\n",
    "    lm = t68[48:49, :]\n",
    "    rm = t68[54:55, :]\n",
    "    t5 = np.concatenate([le, re, no, lm, rm], axis=0)\n",
    "    t5 = t5.reshape(10)\n",
    "    return t5\n",
    "\n",
    "\n",
    "def save_img(img, save_path):\n",
    "    image_numpy = util.tensor2im(img)\n",
    "    util.save_image(image_numpy, save_path, create_dir=True)\n",
    "    return image_numpy\n",
    "\n",
    "opt = TestOptions().parse(is_jupyter=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset [AllFaceDataset] of size 8 was created\n"
     ]
    }
   ],
   "source": [
    "\n",
    "data_info = data.dataset_info()\n",
    "datanum = data_info.get_dataset(opt)[0]\n",
    "folderlevel = data_info.folder_level[datanum]\n",
    "dataloaders = data.create_dataloader_test(opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualizer = Visualizer(opt)\n",
    "iter_counter = IterationCounter(opt, len(dataloaders[0]) * opt.render_thread)\n",
    "testing_queue = Queue(10)\n",
    "ngpus = opt.device_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing gpu  [0]\n",
      "Network [RotateSPADEGenerator] was created. Total number of parameters: 225.1 million. To see the architecture, do print(network).\n"
     ]
    }
   ],
   "source": [
    "render_gpu_ids = list(range(ngpus - opt.render_thread, ngpus))\n",
    "render_layer_list = []\n",
    "for gpu in render_gpu_ids:\n",
    "    opt.gpu_ids = gpu\n",
    "    render_layer = TestRender(opt)\n",
    "    render_layer_list.append(render_layer)\n",
    "\n",
    "opt.gpu_ids = list(range(0, ngpus - opt.render_thread))\n",
    "print('Testing gpu ', opt.gpu_ids)\n",
    "if opt.names is None:\n",
    "    model = TestModel(opt)\n",
    "    model.eval()\n",
    "    model = torch.nn.DataParallel(model.cuda(),\n",
    "                                  device_ids=opt.gpu_ids,\n",
    "                                  output_device=opt.gpu_ids[-1],\n",
    "                                  )\n",
    "    models = [model]\n",
    "    names = [opt.name]\n",
    "    save_path = create_path(create_path(opt.save_path, opt.name), opt.dataset)\n",
    "    save_paths = [save_path]\n",
    "    f = [open(\n",
    "            os.path.join(save_path, opt.dataset + str(opt.list_start) + str(opt.list_end) + '_rotate_lmk.txt'), 'w')]\n",
    "else:\n",
    "    models = []\n",
    "    names = []\n",
    "    save_paths = []\n",
    "    f = []\n",
    "    for name in opt.names.split(','):\n",
    "        opt.name = name\n",
    "        model = TestModel(opt)\n",
    "        model.eval()\n",
    "        model = torch.nn.DataParallel(model.cuda(),\n",
    "                                      device_ids=opt.gpu_ids,\n",
    "                                      output_device=opt.gpu_ids[-1],\n",
    "                                      )\n",
    "        models.append(model)\n",
    "        names.append(name)\n",
    "        save_path = create_path(create_path(opt.save_path, opt.name), opt.dataset)\n",
    "        save_paths.append(save_path)\n",
    "        f_rotated = open(\n",
    "            os.path.join(save_path, opt.dataset + str(opt.list_start) + str(opt.list_end) + '_rotate_lmk.txt'), 'w')\n",
    "        f.append(f_rotated)\n",
    "\n",
    "test_tasks = init_parallel_jobs(testing_queue, dataloaders, iter_counter, opt, render_layer_list)\n",
    "# test\n",
    "landmarks = []\n",
    "\n",
    "process_num = opt.list_start\n",
    "first_time = time.time()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, {'image': tensor([[[[0.2471, 0.2471, 0.2471,  ..., 0.2510, 0.2510, 0.2510],\n",
      "          [0.2471, 0.2471, 0.2471,  ..., 0.2510, 0.2510, 0.2510],\n",
      "          [0.2471, 0.2471, 0.2471,  ..., 0.2510, 0.2510, 0.2510],\n",
      "          ...,\n",
      "          [0.3725, 0.3804, 0.3882,  ..., 0.6863, 0.6824, 0.6745],\n",
      "          [0.4039, 0.4196, 0.4275,  ..., 0.6941, 0.6902, 0.6902],\n",
      "          [0.4431, 0.4549, 0.4784,  ..., 0.6941, 0.6941, 0.6941]],\n",
      "\n",
      "         [[0.3294, 0.3294, 0.3294,  ..., 0.2902, 0.2902, 0.2902],\n",
      "          [0.3294, 0.3294, 0.3294,  ..., 0.2902, 0.2902, 0.2902],\n",
      "          [0.3294, 0.3294, 0.3294,  ..., 0.2902, 0.2902, 0.2902],\n",
      "          ...,\n",
      "          [0.4353, 0.4314, 0.4235,  ..., 0.3020, 0.3137, 0.3255],\n",
      "          [0.4157, 0.4078, 0.4000,  ..., 0.2745, 0.2863, 0.3020],\n",
      "          [0.3804, 0.3765, 0.3804,  ..., 0.2510, 0.2627, 0.2745]],\n",
      "\n",
      "         [[0.1608, 0.1608, 0.1608,  ..., 0.1843, 0.1843, 0.1882],\n",
      "          [0.1608, 0.1608, 0.1608,  ..., 0.1843, 0.1843, 0.1882],\n",
      "          [0.1608, 0.1608, 0.1608,  ..., 0.1843, 0.1843, 0.1882],\n",
      "          ...,\n",
      "          [0.2039, 0.2118, 0.2118,  ..., 0.3020, 0.3020, 0.3020],\n",
      "          [0.2078, 0.2118, 0.2157,  ..., 0.2941, 0.2941, 0.2980],\n",
      "          [0.2039, 0.2078, 0.2196,  ..., 0.2784, 0.2863, 0.2941]]]]), 'param_path': ['./3ddfa/results/Ann_Veneman_0010.txt'], 'M': tensor([[[ 1.4362e+00,  1.7199e-02, -5.7914e+01],\n",
      "         [-1.7199e-02,  1.4362e+00, -3.8482e+01]]]), 'path': ['./3ddfa/example/Images/Ann_Veneman_0010.jpg']})\n"
     ]
    }
   ],
   "source": [
    "for data in enumerate(dataloaders[0]):\n",
    "    print(data)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 256, 256])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[1][\"image\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Cannot handle this data type",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m/home/public/anaconda3/envs/chengbin/lib/python3.6/site-packages/PIL/Image.py\u001b[0m in \u001b[0;36mfromarray\u001b[0;34m(obj, mode)\u001b[0m\n\u001b[1;32m   2644\u001b[0m             \u001b[0mtypekey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"typestr\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2645\u001b[0;31m             \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrawmode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_fromarray_typemap\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtypekey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2646\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: ((1, 1, 256), '<f4')",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-7d5298325ac5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mPIL\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfromarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"image\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/home/public/anaconda3/envs/chengbin/lib/python3.6/site-packages/PIL/Image.py\u001b[0m in \u001b[0;36mfromarray\u001b[0;34m(obj, mode)\u001b[0m\n\u001b[1;32m   2645\u001b[0m             \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrawmode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_fromarray_typemap\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtypekey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2646\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2647\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Cannot handle this data type\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2648\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2649\u001b[0m         \u001b[0mrawmode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: Cannot handle this data type"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "Image.fromarray(data[1][\"image\"][0].numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: unknown error",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-b91a05e1f3d8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m    \u001b[0;31m# data = trainer.get_input(data_i)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m    \u001b[0mstart_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m    \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtesting_queue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblock\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m    \u001b[0mcurrent_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/public/anaconda3/envs/chengbin/lib/python3.6/multiprocessing/queues.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    111\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_rlock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelease\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m         \u001b[0;31m# unserialize the data after having released the lock\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 113\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_ForkingPickler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    114\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mqsize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/public/anaconda3/envs/chengbin/lib/python3.6/site-packages/torch/multiprocessing/reductions.py\u001b[0m in \u001b[0;36mrebuild_cuda_tensor\u001b[0;34m(tensor_cls, tensor_size, tensor_stride, tensor_offset, storage_cls, storage_device, storage_handle, storage_size_bytes, storage_offset_bytes, requires_grad, ref_counter_handle, ref_counter_offset, event_handle, event_sync_required)\u001b[0m\n\u001b[1;32m    117\u001b[0m                 \u001b[0mref_counter_offset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m                 \u001b[0mevent_handle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 119\u001b[0;31m                 event_sync_required)\n\u001b[0m\u001b[1;32m    120\u001b[0m             \u001b[0mshared_cache\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstorage_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstorage_offset_bytes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mStorageWeakRef\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstorage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: unknown error"
     ]
    }
   ],
   "source": [
    " for i, data_i in enumerate(range(len(dataloaders[0]) * opt.render_thread)):\n",
    "    # if i * opt.batchSize >= opt.how_many:\n",
    "    #     break\n",
    "    # data = trainer.get_input(data_i)\n",
    "    start_time = time.time()\n",
    "    data = testing_queue.get(block=True)\n",
    "\n",
    "    current_time = time.time()\n",
    "    time_per_iter = (current_time - start_time) / opt.batchSize\n",
    "    message = '(************* each image render time: %.3f *****************) ' % (time_per_iter)\n",
    "    print(message)\n",
    "\n",
    "    img_path = data['path']\n",
    "    poses = data['pose_list']\n",
    "    rotated_landmarks = data['rotated_landmarks'][:, :, :2].cpu().numpy().astype(np.float)\n",
    "    rotated_landmarks_106 = data['rotated_landmarks_106'][:, :, :2].cpu().numpy().astype(np.float)\n",
    "\n",
    "\n",
    "    generate_rotateds = []\n",
    "    for model in models:\n",
    "        generate_rotated = model.forward(data, mode='single')\n",
    "        generate_rotateds.append(generate_rotated)\n",
    "\n",
    "    for n, name in enumerate(names):\n",
    "        opt.name = name\n",
    "        for b in range(generate_rotateds[n].shape[0]):\n",
    "            # get 5 key points\n",
    "            rotated_keypoints = landmark_68_to_5(rotated_landmarks[b])\n",
    "            # get savepaths\n",
    "            rotated_file_savepath = create_paths(save_paths[n], img_path[b], folderlevel=folderlevel, pose=poses[b])\n",
    "\n",
    "            image_numpy = save_img(generate_rotateds[n][b], rotated_file_savepath)\n",
    "            rotated_keypoints_str = rotated_file_savepath + ' 1 ' + ' '.join([str(int(n)) for n in rotated_keypoints]) + '\\n'\n",
    "            print('process image...' + rotated_file_savepath)\n",
    "            f[n].write(rotated_keypoints_str)\n",
    "\n",
    "            current_time = time.time()\n",
    "            if n == 0:\n",
    "                if b <= opt.batchSize:\n",
    "                    process_num += 1\n",
    "                print('processed num ' + str(process_num))\n",
    "            if opt.align:\n",
    "                aligned_file_savepath = create_paths(save_paths[n], img_path[b], 'aligned', folderlevel=folderlevel, pose=poses[b])\n",
    "                warped = affine_align(image_numpy, rotated_keypoints.reshape(5, 2))\n",
    "                util.save_image(warped, aligned_file_savepath, create_dir=True)\n",
    "\n",
    "            # save 106 landmarks\n",
    "            rotated_keypoints_106 = rotated_landmarks_106[b] # shape: 106 * 2\n",
    "\n",
    "\n",
    "    current_time = time.time()\n",
    "    time_per_iter = (current_time - start_time) / opt.batchSize\n",
    "    message = '(************* each image time total: %.3f *****************) ' % (time_per_iter)\n",
    "    print(message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6.10 64-bit ('chengbin': conda)",
   "language": "python",
   "name": "python361064bitchengbincondaf0f37d4acace4757a51bad258214f3ea"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
